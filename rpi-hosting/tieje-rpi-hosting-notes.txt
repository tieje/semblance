10/29/2021

Setting up 2 rpis to serve a docker swarm.
Ansible will be used to set up the rpis.
    x Install git
    + Clone this repo
    - Install docker, docker tools, and the visualizer. Create a docker swarm. Docker swarm handles automated load-balancing
    - create postgreSQL db and a backup on the second server
    - Set up ufw firewall on the nginx container
    - set up your router to forward to your ip address
    - set up your domain with no-ip service to forward to your address

https://howchoo.com/g/njy4zdm3mwy/how-to-run-a-raspberry-pi-cluster-with-docker-swarm

I'll need to learn the following:

- Ansible
    https://docs.ansible.com/ansible/latest/user_guide/intro_getting_started.html
    https://www.tutorialspoint.com/ansible/index.htm
- Docker Swarm
    https://docs.docker.com/engine/swarm/
- ufw firewall
    https://www.linux.com/training-tutorials/introduction-uncomplicated-firewall-ufw/
- postgreSQL replication
    https://www.postgresql.org/docs/13/runtime-config-replication.html
    https://www.section.io/engineering-education/how-to-replicate-postgresql-database/
- postgreSQL backup
    https://www.postgresql.org/docs/9.1/backup-file.html

I'm going to start with Ansible.
https://docs.ansible.com/ansible/latest/user_guide/intro_getting_started.html

1. I'll need to run a script that installs Ansible and then have Ansible run a separate script.
It runs over SSH protocol by default so I don't need to install ansible, but I will need to add my key through a script.
https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html
The main user guide:
https://docs.ansible.com/ansible/latest/user_guide/index.html
Ansible galaxy library collection:
https://galaxy.ansible.com/
ssh into rpi after adding ~/.ssh/authorized_keys
https://thepi.io/how-to-ssh-into-the-raspberry-pi/
getting started
https://docs.ansible.com/ansible/latest/user_guide/intro_getting_started.html
Ansible command not found
https://stackoverflow.com/questions/63177609/zsh-command-not-found-ansible-after-pip-installing
My ansible can be found at:
ansible --version
/Users/thomasfrancis/.local/bin/ansible
Trying to run the correct ansible command
https://docs.ansible.com/ansible/latest/reference_appendices/general_precedence.html#general-precedence-rules
I need to supply a password
# ansible -u pi -i /Users/thomasfrancis/Documents/ansible/hosts -m ping all --ask-pass -c paramiko
    # does not work
Because I'll need to supply a password for my servers, I'll just create a playbook and connect that way instead.
https://docs.ansible.com/ansible/latest/user_guide/connection_details.html#connections
Decide how you're going to pass in credentials.


rpi1
hostname -I
192.168.1.33
192.168.1.34
public Ip
96.32.2.83
ssh pi@192.168.1.33
ssh pi@192.168.1.34
either one works

rpi2
hostname -I
192.168.1.35
192.168.1.36
public Ip
96.32.2.83
ssh pi@192.168.1.35
ssh pi@192.168.1.36
either one works

+ installed ansible on my mac
+ generate rpi ssh key
+ make draft email on gmail with public ssh key. Copy and paste onto the RPIs
+ removed other versions of python
+ add ansible to path in .zshrc

10/31/2021

I'm going to disable password authentication for the servers.
https://linuxize.com/post/how-to-setup-passwordless-ssh-login/
It seems like it's more secure since key-based authentication relies on cracking the passphrase on your personal key.
https://serverfault.com/questions/334448/why-is-ssh-password-authentication-a-security-risk
I'm gonna add an ssh private key to my mac OS
https://fplanque.com/dev/mac/secure-ssh-private-keys-on-mac-osx-10-5
This worked in solving the password problem. I won't need to enter a password anymore.
Use the --become flag to sudo commands.


ssh pi@192.168.1.35
ssh pi@192.168.1.33

+ register ssh private key with my mac
    ssh-add -k ~/.ssh/rpi
+ ping -m all command on provided inventory
    ansible -u pi -i ~/Documents/yorha/rpi-hosting/hosts -m ping all
+ use a live command on all nodes
    ansible -u pi -i ~/Documents/yorha/rpi-hosting/hosts all -a "/bin/ls"
    # -a is for module arguments
+ change default ansible_python_interpreter from python to python3 as an inventory variable
    https://docs.ansible.com/ansible-core/2.11/reference_appendices/interpreter_discovery.html
    https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#adding-variables-to-inventory
+ set up and run an example ansible playbook
    ansible-playbook ~/Documents/yorha/rpi-hosting/tasks/hello-task.yml -u pi -i ~/Documents/yorha/rpi-hosting/hosts
+ print file name that was created
    /bin/bash ~/Documents/yorha/rpi-hosting/tasks/print-ls-hello-command.sh
+ delete the hello file that was created
    ansible-playbook ~/Documents/yorha/rpi-hosting/tasks/delete-hellofiles.yml -u pi -i ~/Documents/yorha/rpi-hosting/hosts
+ test if file exist (it should not)
+ edit default inventory on mac
    I'm going to export the ansible hosts variable to my hostfile for. I'll change this in the future.
    https://stackoverflow.com/questions/21958727/where-to-store-ansible-host-file-on-mac-os-x
    ansible -i $ANSIBLE_HOSTS -m ping all
    I'll do it this way for now for clarity.
+ Set up pyenv correctly... again
    https://opensource.com/article/20/4/pyenv
+ create user thomasfrancis using ansible playbook
    https://linuxize.com/post/how-to-create-users-in-linux-using-the-useradd-command/
    https://serversforhackers.com/c/create-user-in-ansible
    I'll need to set up a password, otherwise I won't be able to log in remotely.
    https://unix.stackexchange.com/questions/108562/is-there-some-default-password-for-a-new-user-in-linux
    Delete user
    https://linuxize.com/post/how-to-delete-users-in-linux-using-the-userdel-command/
    https://docs.ansible.com/ansible/2.3/user_module.html
    I'll need to encrypt the password upon creation.
    https://www.edureka.co/community/83388/warning-appears-hashed-argument-encrypted-module-properly
    I'll need to install passlib python module.
    Error: crypt.crypt not supported on Mac OS X/Darwin, install passlib python module. crypt.crypt not supported on Mac OS X/Darwin, install passlib python module
    I needed to install passlib locally on my mac, not on the target machine
    It works. User thomasfrancis has the same groups as pi and the same authorized_keys so no password needed to login and I don't need to use -u pi anymore as flag.
    The base commands will be the following from now on:
    ansible -i $ANSIBLE_HOSTS all
    ansible $ANSIBLE_TASKS/[playbook] -i $ANSIBLE_HOSTS

11/1/2021

11/2/2021

+ set up keychain access on Mac
    https://support.apple.com/guide/keychain-access/what-is-keychain-access-kyca1083/mac
    This warning came up:
    The -K and -A flags are deprecated and have been replaced
    by the --apple-use-keychain and --apple-load-keychain
    flags, respectively.  To suppress this warning, set the
    environment variable APPLE_SSH_ADD_BEHAVIOR as described in
    the ssh-add(1) manual page.
    This command worked for me:
    ssh-add --apple-use-keychain ~/.ssh/rpi
+ use an ansible playbook to setup git config ==> I'll use a one-liner instead of setting up git config.
    I'll need it because I'll be cloning from a private repo.
    Looks like I'll use a plugin.
    https://docs.ansible.com/ansible/latest/collections/community/general/git_config_module.html
        + ansible-galaxy collection install community.general
            # already installed
    Storing credentials is not the way.
    Instead I'll run the username and password in the https request.
    https://stackoverflow.com/questions/37841914/how-do-i-pass-username-and-password-while-using-ansible-git-module
    What occurred instead is I used a one-liner https git clone repo with the username and password included.
    This was definitely the simplest solution. I did not need to set up git config at all.
+ create a playbook that clones yorha repo and copies .env folders into their respective places

11/3/2021
I'll need to 

11/4/2021

3pm - currently stuck on how to resolve nginx and docker swarm
https://www.nginx.com/blog/docker-swarm-load-balancing-nginx-plus/
It looks like we'll be using virtual IPs from the swarm (VIPs)
Because of the VIPs, it looks like nginx will be easier to configure and deploy with swarm.
The main goal right now is to simply deploy the build version of only the frontend. This is to test if this even works.
This is highly experimental and not production. However, there are certain things that I will keep in production from this experiment.

Experimental:
- docker hub frontend repo
- nginx configuration
Production:
- docker swarm initialization
- ufw settings
- monitoring if its set up
- semblance.us DNS configuration
- no-ip DNS configuration

I'll need to do these tasks in the following order:

+ fork the current yorha repo into a new semblance repo
    # worry about the .env files later since you're only working with the frontend
    # I couldn't fork it, so I had to change the folder name and add large folders one a time

11/5/2021


+ use docker swarm on your rpis
    https://docs.docker.com/get-started/orchestration/
    https://docs.docker.com/get-started/swarm-deploy/
    https://docs.docker.com/engine/swarm/
    https://docs.docker.com/engine/swarm/key-concepts/
    https://docs.docker.com/engine/swarm/swarm-tutorial/
    I turns out that I will need ansible to install docker engine on the rpis. I wish I just studied docker swarm instead of using nginx.
    https://docs.docker.com/engine/install/debian/
    I will not be using Kubernetes due to its complexity.
    https://www.ibm.com/cloud/blog/docker-swarm-vs-kubernetes-a-comparison
    I think I'll use the guide below. 
    https://howchoo.com/g/njy4zdm3mwy/how-to-run-a-raspberry-pi-cluster-with-docker-swarm
    + change the hostname of your rpis. It's significantly easier to do this by hand
        https://howchoo.com/pi/how-to-change-the-hostname-of-your-raspberry-pi
        sed -i 's/raspberrypi/manager1/g'
        https://linuxize.com/post/how-to-use-sed-to-find-and-replace-string-in-files/
        There are three types of hostnames: user-defined, static (kernel hostname), and transient (network hostname)
        https://www.redhat.com/sysadmin/change-hostname-linux
        + rename hostname of .33 rpi to manager1
        + rename hostname of .35 rpi to worker1
    x create a playbook that installs docker engine on each node
        https://docs.docker.com/engine/install/debian/
        raspbian is an exception where we will need to install docker using the convenience script
        https://docs.docker.com/engine/install/debian/#install-using-the-convenience-script
        curl -fsSL https://get.docker.com -o get-docker.sh
        OR
        curl -sSL https://get.docker.com | sh
        I'm going to abandon the playbook in favor of installing it manually myself for now.
    + install docker using the curl commands above
    + install python sdk for docker and docker-compose from pip
        pip3 install docker docker-compose
    x create an ansible command for docker swarm init
        since we'll be using the manager node, it's not really worth it to build an ansible command for this
    + add my user to the docker group on all nodes so I can run without root privileges
        sudo usermod -aG docker thomasfrancis
    + create docker swarm with init command on manager1
        docker swarm init --advertise-addr 192.168.1.33
    + copy the output for adding workers here
        docker swarm join --token SWMTKN-1-3t5ux1mlm3fr68tdmefgd36q8l6aoblzi343dyp8kcrfgt57qf-3ztfpdcadcslb9oc8i623wmd3 192.168.1.33:2377
    + add worker1 as a worker node
    + add the visualizer service
        upon restarting both nodes, it turns out the swarm does automatically startup which may mean the same for services.
        It doesn't seem like services have too much of the overhead of containers.
        http://192.168.1.33:8080
        command used:
        sudo docker service create \
                --name viz \
                --publish 8080:8080/tcp \
                --constraint node.role==manager \
                --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \
                alexellis2/visualizer-arm:latest
+ have no-ip address point to the rpi IP address + the appropriate port/domain name
    http://semblance.ddns.net/
    had to set up port forwarding from my router. My IP address is likely dynamic so I'll need to update my changes.
+ have semblance.us either point or forward to the no-ip address
+ edit nginx configuration to point to the VIP of the frontend
    ~ study the nginx cookbook to determine what the configuration should look like
    This is possible because the VIP of containers is not only predictable, but also configurable
    https://docs.docker.com/engine/swarm/swarm-mode/
    https://www.thepolyglotdeveloper.com/2017/05/load-balancing-docker-swarm-cluster-nginx-reverse-proxy/
    The default address pool for docker swarm is 10.0.0.0/8
    I'll use this default pool for now.
    The swarm is set up. The visualizer is already running on port 8080 so I'll the nginx service on port 8081
    I figured out most of the nginx part. Instead of port forwarding to port 8080 on my router, I'll need to forward to port 80 since this is what nginx is listening on.
    I'll create a separate server configuration for the visualizer since I like it.
    One way that this method would not work is there were many servers.
    For now, I don't really need to use the VIP of services.
    If I were to truly scale, I would use the VIP of services
- upload only the nginx and frontend images to the docker hub repo
    ~ learn how to update the docker hub repo image over time
        # I'm guessing that all I need to do is rebuild the image and keep pushing it up
    x create the docker service statements or the stack deploy statements
        x it seems like all I really need is a single .yml file for a stack deploy statement
            scp docker-compose.prod.yml thomasfrancis@192.168.1.33:/home/thomasfrancis
            It's not enough. I'll need the dockerfile to actually get anything in there.
        https://stackoverflow.com/questions/48962399/no-suitable-node-unable-to-deploy-image-using-docker-service
        docker stack deploy --compose-file docker-compose.prod.yml semblance 
        https://docs.docker.com/engine/reference/commandline/stack_deploy/
        There are 0/1 replicas so that's not good.
        https://florianmuller.com/build-a-raspberry-pi-4-docker-swarm-cluster-with-four-nodes-and-deploy-traefik-with-portainer
        I think I'll try using services instead of stack deploy
    x create the docker service statement
        https://docs.docker.com/engine/reference/commandline/service_create/
        The main issue is that the node label must be updated for the designation of running such services.
        The service can run on both manager1 and worker1 so I'll add those labels.
        https://stackoverflow.com/questions/42414703/how-to-list-docker-swarm-nodes-with-labels

The experiment has failed. I've given up due to incompatibility issues.
I was really hoping that I could get it off the ground, but I've determined a simpler solution without using docker swarm.
In the future, I'll need to practice creating docker swarm on my linux desktop just to see if things will work.
The rpi architecture and my own architecture has just gotten in the way... too much.

docker service create \
--replicas 1 \
--name semblance_frontend \
-p 5001:5001 \
tieje/frontend

docker service create \
--replicas 1 \
--name semblance_nginx \
-p 5001:5001 \
tieje/nginx

The main issue is likely a dependency issue. Certain packages don't translate well on the RPI.
I guess that part should not matter...
I want to give up, but somehow the visualizer is working. And I don't know how.

I'm going to try to run the nginx image directly on the pi and see what happens.

docker run -p 5001:5001 -d --platform=linux/arm/v7 tieje/frontend "npm i && && serve -l 5001 -s build"
docker run -it -p 80:80 -d --platform=linux/arm/v7 tieje/frontend /bin/bash

docker run -p 80:80 -d --platform=linux/arm/v7 tieje/nginx /bin/bash
docker exec -it 46fcc926a500 /bin/bash

I received the following error. I rebuilt the image and pushed it to docker hub.
In retrospect, I could have just specified it here in the docker run command
WARNING: The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/arm/v7) and no specific platform was requested
https://docs.docker.com/desktop/multi-arch/
I'm going to build a builder so that I can build for linux/arm/v7
docker buildx build --platform linux/arm/v7 -t tieje/reverseproxy:latest --push ./nginx
I created a new builder called Armv7builder. I didn't give it any specifications but it appears to build for other architectures.
docker buildx build --platform linux/arm/v7 -t tieje/frontend:latest --push ./Frontend

docker exec -it fcde6ea3278d /bin/bash
ran npm start from inside. I curled the localhost:3000 but it did not work...
I'll edit the Dockerfile with a command to run npm start or the serve command.
I need to know the difference between "RUN" and "CMD" in the Dockerfile

I want to try pushing the compose. That might make a difference.
I'm trying to make my Dockerfiles run as standalone containers on their own now. I guess only the frontend.
I need to rethink how I build Dockerfiles
I want to try running docker-compose create since it creates a service for a docker swarm.
docker-compose create command is deprecated.
docker stack deploy --compose-file docker-compose.prod.yml semblance
And I was gonna give up. It works now. Well that was easy.
I have no idea how the builder works, but it was integral to getting this done correctly.
I'll need to comment out the container for now so that the load balancer works.

docker buildx build --platform linux/arm/v7 -t tieje/reverseproxy:2 --push ./nginx
docker buildx build --platform linux/arm/v7 -t tieje/frontend:2 --push ./Frontend
I think pushing it to dockerhub works, but I'll need to something else to update it locally.
https://stackoverflow.com/questions/50936208/how-can-i-update-the-latest-image-that-my-docker-service-stack-uses
What I'm looking for is optimizing performance of nginx:
https://stackoverflow.com/questions/54059581/deploy-react-with-nginx-failed-to-load-resources
https://docs.nginx.com/nginx/admin-guide/web-server/serving-static-content/
turning on sendfile, tcp_noopush and tcp_nodelay
Minimal performace enhancement.
The real issue is that I'm running a real build.
Didn't know I could do this: https://mherman.org/blog/dockerizing-a-react-app/
https://bogotobogo.com/DevOps/Docker/Docker-React-App.php
It looks like everyone has been doing it the same way.
And it makes sense. I do something very similar with the backend where gunicorn handles hosting.
It's likely that I've been going about this the wrong way the whole time.
I'll try it the "right" way.
docker build -f Dockerfile -t tieje/frontend:2 .
I learned so much today it's pretty insane. But hey. It works. And I'm not afraid to implement the nginx reverse proxy later. I'm sooooo taking a day off tomorrow.
11/10/2021

Post-Experiment: Production build

The order of deployment will be:
1. Database
2. Backend
3. Reversse-proxy
4. Firewall

11/11/2021

x get postgresql running on rpis using ansible
+ set up postgresql db on worker1
    x create a playbook that installs postgresql
    x create a playbook that creates the user on postgresql necessary for docker
    x create a playbook that creates the database and starting data ONLY ON ONE NODE
    x create a playbook that replicates this data from one database to another
    I probably won't create any playbooks mostly because I don't really need to.
    This is a decent guide. I'll need to see what others say.
    https://kb.objectrocket.com/postgresql/how-to-install-and-set-up-postgresql-on-a-raspberry-pi-part-2-1165
    This is a more basic method. It's less about messing around with configuration and network files though.
    I don't think I'll be using it.
    https://pimylifeup.com/raspberry-pi-postgresql/
    This is a decent guide because it actually explains what things means while setting up a configurations.
    https://bpwalters.com/blog/setting-up-postgresql-on-raspberry-pi/
    The best thing to do, might actually be to dedicate an entire RPI for all my database needs.
    In other words, I should be able to access that database from a backend container running on my laptop.
    This would allow me to experiment with the backend configuration without setting up a container on the RPI itself.
    I won't need the pgAdmin GUI.
    I believe that I've determined the resources that I'll use. I need to answer the following questions now:
    Why install postgresql-contrib as well?
    Why check the status of Postgres clusters with the pg_clusters command.
    Why copy pg_hba.conf path?
        This controls client authentication.
        We're whitelisting IP addresses to allow access.
    What does the postgresql.conf file do?
        Again, we're whitelisting IP addresses.
        This allows the postgresql server to accept connections from my IPs.
    postgresql.conf specifies a list of addresses so the server accepts incoming connections from specified IPs
    pg_hba.conf controls access at a finer grained level for which IPs the server will accept logins from for specific dbs and users
    postgresql.conf is for general traffic.
    pg_hba.conf is for who is allowed direct control from login.
    Generally, it seems like a good idea to whitelist all of my IPs on my network just in case I get any new computers.
    I only accept and forward port 80 external connections anyways so it should be relatively secure.
    The only way for someone to get into the database is for them join the network on my router.
    postgresql-contrib is contributed extensions and additions to postgresql.
    I will be needing extensions like gis, so perhaps I may need to install even more...
    postgis makes this a bit more difficult actually.
    + build out the installation command
        How to install postgis on postgresql
        https://raspberrypi.stackexchange.com/questions/71144/installing-postgis-on-raspberry-pi-to-postgresql-9-6
        On libpq-dev:
        https://pypi.org/project/libpq-dev/
        sudo apt-get install postgresql postgresql-contrib postgis libpq-dev -y
    + run installation command on worker1. This will be the dedicated postgreSQL db server for all my applications.
        It's installed but it's not running, which is fine.
    + run postgresql
        pg_lsclusters
        location of the log just in case you need to view it if the db cluster goes down or is having problems
        /var/log/postgresql/postgresql-11-main.log
        data directory
        /var/lib/postgresql/11/main
        pg_hba.conf
        /etc/postgresql/11/main/pg_hba.conf
        postgresql.conf
        /etc/postgresql/11/main/postgresql.conf
    + make a backup of both original configurations
        sudo cp [original config] [original config .bak]
        sudo cp /etc/postgresql/11/main/pg_hba.conf /etc/postgresql/11/main/pg_hba.conf.bak
        sudo cp /etc/postgresql/11/main/postgresql.conf /etc/postgresql/11/main/postgresql.conf.bak
    + edit the pg_hba.conf and postgres.conf files
        I have two password security layers protecting access to the database. I think that's good enough.
        sudo emacs /etc/postgresql/11/main/pg_hba.conf
        grep -n "listen_addresses =" /etc/postgresql/11/main/postgresql.conf
        sudo sed -i "s/listen_addresses = 'localhost'/listen_addresses = '*'/" /etc/postgresql/11/main/postgresql.conf
        sudo emacs /etc/postgresql/11/main/postgresql.conf
    + reload the server
        sudo service postgresql restart
+ set up the backend to use the worker1 db
    This gives a pretty succinct answer for why PostgreSQL is better than MySQL
    https://www.askpython.com/django/django-postgresql
    I have already created the password and saved it on my password manager and semblance secrets.
    I'll need to create a user and a database. The user will need the appropriate permissions that django will need.
    + create a user called thomas
        sudo -u postgres psql
        alter role postgres with password 'eLEO4Wo6S3ZSe5OXLFS%VK';
        CREATE USER thomas WITH LOGIN PASSWORD 'eLEO4Wo6S3ZSe5OXLFS%VK';
    + add roles to user thomas
        alter role thomas with password 'eLEO4Wo6S3ZSe5OXLFS%VK';
        alter role thomas with superuser;
    + create a database called semblance_db
        CREATE DATABASE semblance_db WITH OWNER=thomas;
        I had to change pg_hba.conf from peer to md5
    + change database settings backend
    + create new docker-compose file just to run the backend
    + troubleshoot database connection for django
        It turns out that postgresql only accepts unix-domain connections by default
        http://nst.sourceforge.net/nst/docs/faq/ch06s02.html
        sudo grep -n "PGOPTS" /etc/postgresql/11/main/postgresql.conf
        sudo grep -n "PGPORT" /etc/postgresql/11/main/postgresql.conf
        sudo grep -n "PORT" /etc/postgresql/11/main/postgresql.conf
        It looks like it doesn't exist as a setting.
        Before I make the changes, I'd like to try some things.
        I think postgresql is listening to the right port, it has the right name, and tcp/ip connection should be allowed.
        I'm going to install ufw and open up the port.
        https://pimylifeup.com/raspberry-pi-ufw/
        psql -d semblance_db -U thomas
        This command helped me figure out that I did not uncomment out listen_addresses in the postgresql.conf file.
        show listen_addresses;
+ create the nginx configuration file for the frontend such that it points to the backend and visualizer
    https://stackoverflow.com/questions/52073803/how-to-do-nginx-reverse-proxy-to-docker-services-app-created-in-docker-swarm
    The alternative solution is over-engineered.
    The easier solution would be to simply port forward connections to ports 8000 and 8080.
    I didn't really understand why nginx -g daemon off command was used.
    https://stackoverflow.com/questions/18861300/how-to-run-nginx-within-a-docker-container-without-halting
    Decent article. Docker has advanced networking capabilities that I should take advantage of.
    https://semaphoreci.com/community/tutorials/consuming-services-in-a-docker-swarm-mode-cluster
    I could either create an overlay network and run the backend service separately from the frontend.
    OR I could take advantage of multi-stage builds to run the frontend and backend all in one container.
    It's not a good idea to combine the frontend and backend because docker containers were meant to do only one thing each.
    Another method would be to use the Swarm load balancer itself to connect to replicas, which would be preferable...
    There are several ways to do this, but I feel like the most organic would to use the real IP address.
x rebuild the semblance service to include the backend
    docker-compose -f docker-compose.prod.yml up -d --build
    docker buildx build --platform linux/arm/v7 -t tieje/frontend:5 --push ./Frontend
    docker buildx build --platform linux/arm/v7 -t tieje/backend:3 --push ./Backend
    scp docker-compose.prod.yml thomasfrancis@192.168.1.33:/home/thomasfrancis
    scp docker-compose.prod.yml thomasfrancis@192.168.1.33:/home/thomasfrancis/semblance
    docker stack rm semblance
    cp -r $HOME/semblance-secrets/Backend/env/ $HOME/semblance/Backend/
    cp docker-compose.prod.yml $HOME/semblance/docker-compose.prod.yml
    docker stack deploy --compose-file docker-compose.prod.yml semblance
    Early testing with the visualizer shows that it actually works.
    x troubleshoot django
        It turns out that django doesn't serve files itself.
        https://www.digitalocean.com/community/questions/digital-ocean-django-nginx-error-not-found-the-requested-resource-was-not-found-on-this-server
        I totally forgot that django and nginx don't really go well together.
        https://medium.com/analytics-vidhya/dajngo-with-nginx-gunicorn-aaf8431dc9e0
        https://mattsegal.dev/nginx-django-reverse-proxy-config.html
        I'll need to troubleshoot it some more.
        I feel like proxy passing isn't worth it. Unfortunately, there doesn't seem to be a way around it.
        I have a sinking feeling that I won't be able to figure this one out. :(
        10/12/2021
        I'll keep the above method in mind, but this is something that I'll do later.
        + reserve ip addresses on your router. For some reason, some IP addresses get reassigned.
        I'm thinking of performing a multi-stage build for the backend.
        I can't rely on the nginx of the frontend container to serve the files of the backend.
        It seems like I might need to use ansible to keep a copy of the application on all servers.
        This configuration may be more complex than what I actually need.
        https://uwsgi-docs.readthedocs.io/en/latest/tutorials/Django_and_nginx.html
        I like what's happening here.
        https://mattsegal.dev/nginx-django-reverse-proxy-config.html
        The api needs to be free-standing, so I will definitely be performing a multi-stage build with nginx.
        nginx web server documentation
        https://docs.nginx.com/nginx/admin-guide/web-server/web-server/
        + remove backend from the frontend nginx.conf
        + create backend nginx.conf
        + create multi-stage build docker file so that nginx will serve wsgi
        x test locally with docker-compose.backend.yml. Does api.localhost work?
        Experiment failed. It's not a particularly good idea at all since I'll need to set up a reverse proxy anyways.
        + use the frontend container as a reverse proxy for the backend
            The experiment was successfully locally with docker compose.
            I might just need to figure out how docker swarm services can communicate and have nginx be forwarded to the right service.
        x figure out how to allow nginx to connect to a service
            This might be a viable solution.
            https://tech-related.com/p/VfaQyu9S4M
            Informational about how the docker swarm ingress overlay network works.
            https://stackoverflow.com/questions/46546497/how-to-change-the-service-name-generated-by-docker-stack-in-docker-compose
            It might just work out of the box already.
                docker-compose -f docker-compose.prod.yml up -d --build
                docker buildx build --platform linux/arm/v7 -t tieje/frontend:6 --push ./Frontend
                docker buildx build --platform linux/arm/v7 -t tieje/backend:4 --push ./Backend
                docker stack deploy --compose-file docker-compose.prod.yml semblance
            It connects to the backend... somehow. Not sure how I got this through, but the functionality is incomplete.
            It's incomplete for the backend. Rather than reverse proxying to another container, maybe I should try all in one? All in one defeats the purpose of containers...
            "It is recommended that you use separate overlay networks for each application or group of applications which will work together."
            https://docs.docker.com/network/network-tutorial-overlay/
            docker network ls shows that there is already a semblance_default overlay network running.
            No need to create a new network. I just need to use the semblance_default overlay network.
            docker network inspect shows me that the default network only has the backend container.
            I'll need to create my own overlay network after all.
            The overlay network is not something I should create with a docker-compose file...
            https://docs.docker.com/network/overlay/
            docker network create -d overlay --attachable semblance-net
            docker service update --network-add semblance-net semblance_backend
            docker service update --network-add semblance-net semblance_frontend
            docker service update --image tieje/backend:7 semblance_backend
            docker service update --image tieje/frontend:6 semblance_frontend
            docker buildx build --platform linux/arm/v7 -t tieje/backend:7 --push ./Backend
            It's weird.
            As much as I don't like it, I'm going to need to start over and try other approaches.
            It may be a super simple fix. I perhaps all I'll need to do is serve static files from nginx.
            It's possible that I'm able to get away with api.localhost is because the container has all the resources it needs.
            A service differs from a container in that there is no stable localhost. The gunicorn wsgi cannot load in time to serve new instances upon reloads. I turned off some nginx settings ensure it loads but that won't be enough.
            I ran into a similar problem when I was using a reverse proxy for the frontend.
            Realistically, this problem mainly occurred because I was not running the build version of frontend.
            The frontend is already basically a reverse proxy.
            https://stackoverflow.com/questions/40871089/should-nginx-be-packed-into-the-same-container-as-django-when-deploying-with-doc
            I'll make it so that the backend is a standalone container with nginx.
            gunicorn will serve the back end while nginx will serve the static files.
            I can't use a multi-stage build since I'll be needing all files.
            It looks like I'll be install nginx directly.
            I guess I will be using a multi stage build.
            https://www.tutorialspoint.com/combine-multiple-images-using-one-dockerfile
        Lots of experimenting right now with nginx. Starting from scratch was a good idea.
Experiments have been successful.
The frontend still lives.
- deploy semblance stack on docker swarm
    + push the backend and reverseproxy images to dockerhub
        docker buildx build --platform linux/arm/v7 -t tieje/backend:13 --push ./Backend
        docker buildx build --platform linux/arm/v7 -t tieje/frontend:11 --push ./Frontend
    + remove old semblance stack and network
        docker stack rm semblance
    + push changes to git and pull them on the rpis
    + update/deploy new stack
        docker stack deploy --compose-file docker-compose.prod.yml semblance 
        docker service update --image tieje/backend:13 semblance_backend
        docker service update --image tieje/frontend:10 semblance_frontend
        Instead of an upstream backend, I might just use the gunicorn port directly
        https://serverfault.com/questions/960131/nginx-docker-and-gunicorn-url-without-the-port-number-in-the-url
        + copy correct .env files from semblance secrets
            cp -r $HOME/semblance-secrets/Database/env/ $HOME/semblance/Database/

I'm going to stop here for now since the ssl certificate is not that important.
Also I need a change of pace. I've been working on this for about two weeks now.

- learn how to get an ssl certificate and install it
    https://certbot.eff.org/lets-encrypt/debianbuster-nginx
    I wonder if I'll need to set up the ssl cert on the rpi or the nginx container?
    https://stackoverflow.com/questions/26028971/docker-container-ssl-certificates
    I guess they use a volume for certs... but that doesn't seem right. I'll need to figure out how ssl certs work?
    The big idea is that the ssl cert should be a reflection of the physical server not, the container.
    https://stackoverflow.com/questions/52244214/installing-ssl-cert-in-docker-swarm
    https://scalified.com/2018/10/08/building-jenkins-pipelines-docker-swarm/
    This is one method:
    https://finnian.io/blog/ssl-with-docker-swarm-lets-encrypt-and-nginx/
=============
BASE COMMANDS
=============
ansible -i $ANSIBLE_HOSTS all 
ansible-playbook $ANSIBLE_TASKS/[playbook] -i $ANSIBLE_HOSTS

===============
COMMON COMMANDS
===============
http://192.168.1.33:8080
ssh thomasfrancis@192.168.1.33
ssh thomasfrancis@192.168.1.35
sudo shutdown now
ansible -i $ANSIBLE_HOSTS all -m ping
sudo shutdown -r now

============
EXPORT PATHS
============

export ANSIBLE_HOSTS="$HOME/Documents/yorha/rpi-hosting/hosts"
export ANSIBLE_TASKS="$HOME/Documents/yorha/rpi-hosting/tasks"
